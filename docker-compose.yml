services:
  
  # run 'docker exec batchai.ollama ollama pull [qwen2.5-coder:7b-instruct-fp16]' to pull specific models
  ollama:
    image: ollama/ollama:0.3.12
    container_name: batchai.ollama
    restart: unless-stopped
    volumes:
      - ./work/ollama:/root/.ollama
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
  
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: batchai.ollama.open-webui #admin/admin@example.com/pwd
    restart: unless-stopped
    ports:
      - "3000:8080"
    volumes:
      - ./work/ollama.open-webui:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - HTTPS_PROXY=http://192.168.4.31:10081
  